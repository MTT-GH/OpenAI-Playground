{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Computer Vision and Lainchain\n",
    "\n",
    "**Azure Computer Vision** is a cloud-based service that provides advanced algorithms for processing images and returning information based on the visual features you’re interested in. You can use it to extract text, analyze faces, moderate content, generate captions, and more. You can also run it on the edge, in containers, for scenarios that require data security and low latency.\n",
    "\n",
    "**LangChain** is an open-source framework that simplifies the creation of applications using large language models (LLMs). It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. You can use it to connect a language model to other sources of data, and allow it to interact with its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this Notebook\n",
    "\n",
    "Make sure you have created an Azure Computer Vision resource and added the following settings to the GitHub Codespace secrets in your repo:\n",
    "\n",
    "- dsfs\n",
    "- fsdfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ai.vision as visionsdk\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import AzureOpenAI\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all settings from GitHub Codespace secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_COMPUTER_VISION_ENDPOINT = os.getenv(\"AZURE_COMPUTER_VISION_ENDPOINT\")\n",
    "AZURE_COMPUTER_VISION_KEY = os.getenv(\"AZURE_COMPUTER_VISION_KEY\")\n",
    "\n",
    "OPENAI_API_BASE = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_CHAT_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Azure Computer Vision to analyze images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_url):\n",
    "    \"\"\"\n",
    "    Azure computer vision analysis\n",
    "    \"\"\"\n",
    "    service_options = visionsdk.VisionServiceOptions(\n",
    "        AZURE_COMPUTER_VISION_ENDPOINT, AZURE_COMPUTER_VISION_KEY\n",
    "    )\n",
    "    vision_source = visionsdk.VisionSource(url=image_url)\n",
    "    analysis_options = visionsdk.ImageAnalysisOptions()\n",
    "\n",
    "    analysis_options.features = (\n",
    "        visionsdk.ImageAnalysisFeature.CROP_SUGGESTIONS\n",
    "        | visionsdk.ImageAnalysisFeature.CAPTION\n",
    "        | visionsdk.ImageAnalysisFeature.DENSE_CAPTIONS\n",
    "        | visionsdk.ImageAnalysisFeature.OBJECTS\n",
    "        | visionsdk.ImageAnalysisFeature.PEOPLE\n",
    "        | visionsdk.ImageAnalysisFeature.TEXT\n",
    "        | visionsdk.ImageAnalysisFeature.TAGS\n",
    "    )\n",
    "\n",
    "    analysis_options.language = \"en\"\n",
    "    analysis_options.model_version = \"latest\"\n",
    "    analysis_options.gender_neutral_caption = True\n",
    "\n",
    "    image_analyzer = visionsdk.ImageAnalyzer(\n",
    "        service_options, vision_source, analysis_options\n",
    "    )\n",
    "    result = image_analyzer.analyze()\n",
    "\n",
    "    if result.reason == visionsdk.ImageAnalysisResultReason.ANALYZED:\n",
    "        print(\" Image url:\", image_url)\n",
    "        print(\" Image height: {}\".format(result.image_height))\n",
    "        print(\" Image width: {}\".format(result.image_width))\n",
    "        print(\" Model version: {}\".format(result.model_version))\n",
    "\n",
    "        if result.caption is not None:\n",
    "            print()\n",
    "            print(\" Caption:\")\n",
    "            print(\n",
    "                \"   '{}', Confidence {:.4f}\".format(\n",
    "                    result.caption.content, result.caption.confidence\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if result.dense_captions is not None:\n",
    "            print()\n",
    "            print(\" Dense Captions:\")\n",
    "            for caption in result.dense_captions:\n",
    "                print(\n",
    "                    \"   '{}', {}, Confidence: {:.4f}\".format(\n",
    "                        caption.content, caption.bounding_box, caption.confidence\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if result.objects is not None:\n",
    "            print()\n",
    "            print(\" Objects:\")\n",
    "            for object in result.objects:\n",
    "                print(\n",
    "                    \"   '{}', {}, Confidence: {:.4f}\".format(\n",
    "                        object.name, object.bounding_box, object.confidence\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if result.tags is not None:\n",
    "            print()\n",
    "            print(\" Tags:\")\n",
    "            for tag in result.tags:\n",
    "                print(\"   '{}', Confidence {:.4f}\".format(tag.name, tag.confidence))\n",
    "\n",
    "        if result.people is not None:\n",
    "            print()\n",
    "            print(\" People:\")\n",
    "            for person in result.people:\n",
    "                print(\n",
    "                    \"   {}, Confidence {:.4f}\".format(\n",
    "                        person.bounding_box, person.confidence\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if result.crop_suggestions is not None:\n",
    "            print()\n",
    "            print(\" Crop Suggestions:\")\n",
    "            for crop_suggestion in result.crop_suggestions:\n",
    "                print(\n",
    "                    \"   Aspect ratio {}: Crop suggestion {}\".format(\n",
    "                        crop_suggestion.aspect_ratio, crop_suggestion.bounding_box\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if result.text is not None:\n",
    "            print()\n",
    "            print(\" Text:\")\n",
    "            for line in result.text.lines:\n",
    "                points_string = (\n",
    "                    \"{\"\n",
    "                    + \", \".join([str(int(point)) for point in line.bounding_polygon])\n",
    "                    + \"}\"\n",
    "                )\n",
    "                print(\n",
    "                    \"   Line: '{}', Bounding polygon {}\".format(\n",
    "                        line.content, points_string\n",
    "                    )\n",
    "                )\n",
    "                for word in line.words:\n",
    "                    points_string = (\n",
    "                        \"{\"\n",
    "                        + \", \".join(\n",
    "                            [str(int(point)) for point in word.bounding_polygon]\n",
    "                        )\n",
    "                        + \"}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        \"     Word: '{}', Bounding polygon {}, Confidence {:.4f}\".format(\n",
    "                            word.content, points_string, word.confidence\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        result_details = visionsdk.ImageAnalysisResultDetails.from_result(result)\n",
    "\n",
    "    else:\n",
    "        error_details = visionsdk.ImageAnalysisErrorDetails.from_result(result)\n",
    "        print(\" Analysis failed.\")\n",
    "        print(\"   Error reason: {}\".format(error_details.reason))\n",
    "        print(\"   Error code: {}\".format(error_details.error_code))\n",
    "        print(\"   Error message: {}\".format(error_details.message))\n",
    "        print(\" Did you set the computer vision endpoint and key?\")\n",
    "\n",
    "    return result_details.json_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s test it with the following image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://www.awanireview.com/wp-content/uploads/2021/06/Play-Xbox-X-Series-games-on-your-Xbox-One-with-2048x1224.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image_data = BytesIO(response.content)\n",
    "img = Image.open(image_data)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the shown picture with Azure Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_result = analyze_image(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take only the given captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = json.loads(json_result)\n",
    "dense_captions = result_dict[\"denseCaptionsResult\"][\"values\"]\n",
    "text = \"\\n\".join(caption[\"text\"] for caption in dense_captions)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the use of OpenAI models and Langchain to orchestrate the call to the models, we will generate a Twitter(X) post based on the detected captions\n",
    "\n",
    "**replace** model name if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_DEPLOYMENT_NAME = \"gpt-35-turbo-unai\"\n",
    "OPENAI_MODEL_NAME = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of a class named \"AzureOpenAI\" and assigns it to a variable named \"llm\". The \"AzureOpenAI\" class is initialized with several parameters including the deployment name, model name, API base URL, API key, and API version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "    model_name=OPENAI_MODEL_NAME,\n",
    "    openai_api_base=OPENAI_API_BASE,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_version=OPENAI_API_VERSION,\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation_from_image_AI_insights():\n",
    "    \"\"\"\n",
    "    Text generation from image AI insights\n",
    "    \"\"\"\n",
    "    template = \"\"\"Generate a Tweeter post based on the list of sentences provided below.\n",
    "    The Tweeter post must be {length} characters long. Use the objects described in the list for the Tweeter post.\n",
    "    The Tweeter post must have a strong marketing message, it must have some emoticons and hashtags.\n",
    "\n",
    "    Sentences:\"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(input_variables=[\"length\"], template=template)\n",
    "    prompt_template.format(length=700)\n",
    "    #\n",
    "    langchain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    # Give the Azure Computer Vision captions as input to the LLM  model\n",
    "    generated_txt = langchain.run(text)\n",
    "\n",
    "    print(\"Marketing post provided by Azure AI:\")\n",
    "    print(\"\\033[1;31;34m\", generated_txt)\n",
    "text_generation_from_image_AI_insights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
